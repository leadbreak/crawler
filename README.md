# crawler
code for crawling and saving data as a txt, csv, xls, or DB

# 다양한 사이트의 정보를 크롤링하기 위한 코드입니다.

기초적인 수준의 크롤러부터, 좀 더 복잡하고 디테일한 수준의 작업까지 자동으로 이를 처리하고 실행파일까지 만드는 등의 과정이 있을 예정입니다.

결과물은 다음과 같습니다.


# 1. pyinstaller를 사용해 지정한 카테고리, 키워드 별로 매일 자동적인 데이터 수집과 mysql DB에 저장

![image](https://user-images.githubusercontent.com/71580035/102597048-63610a00-415d-11eb-9fbf-086f5a561c10.png)


# 2. 이렇게 수집한 카테고리를 txt, csv, excel 등으로 저장


## 날짜별로 저장된 폴더
![image](https://user-images.githubusercontent.com/71580035/102597281-c2268380-415d-11eb-9ff7-b5f20f240f03.png)


## 지정한 키워드로 크롤링된 내용 - csv
![image](https://user-images.githubusercontent.com/71580035/102597386-e7b38d00-415d-11eb-9388-2bff1d9cf65e.png)


## 지정한 키워드로 크롤링된 내용 - xls
![image](https://user-images.githubusercontent.com/71580035/102597596-3cef9e80-415e-11eb-8c91-54e6ae2d0888.png)


# 3. 최종적으로 이렇게 수집한 내용들을 db에도 저장
![image](https://user-images.githubusercontent.com/71580035/102597974-b2f40580-415e-11eb-9c2e-33ad40ab49bb.png)



## 궁금하신 사항이 있다면 leadbreak013@gmail.com 으로 메일 주시면 됩니다.
